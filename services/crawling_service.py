# services/crawling_service.py
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
from datetime import datetime
from typing import List, Dict, Optional

# config.pyê°€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆë‹¤ê³  ê°€ì •
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import CRAWLING_TARGET_CATEGORIES, CRAWLING_ISSUES_PER_CATEGORY

class BigKindsCrawler:
    """BigKinds ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ì„œë¹„ìŠ¤"""

    def __init__(self, headless: bool = True):
        self.target_categories = CRAWLING_TARGET_CATEGORIES
        self.issues_per_category = CRAWLING_ISSUES_PER_CATEGORY
        self.headless = headless
        self.driver = None
        self.wait = None

    def _setup_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        options = webdriver.ChromeOptions()
        if self.headless:
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
        
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        try:
            self.driver = webdriver.Chrome(options=options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.wait = WebDriverWait(self.driver, 15)
            print("âœ… WebDriverê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ WebDriver ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def _cleanup_driver(self):
        """WebDriver ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            self.wait = None
            print("âœ… WebDriverê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    def crawl_all_categories(self) -> List[Dict]:
        """ëª¨ë“  ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§"""
        print(f"ğŸš€ BigKinds ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ëŒ€ìƒ: {', '.join(self.target_categories)})")
        self._setup_driver()
        all_issues = []
        
        try:
            self.driver.get("https://www.bigkinds.or.kr/")
            print("ğŸŒ BigKinds ì›¹ì‚¬ì´íŠ¸ì— ì ‘ì†í–ˆìŠµë‹ˆë‹¤.")
            time.sleep(3)
            
            total_issue_id = 1
            for category in self.target_categories:
                print(f"ğŸ“‚ '{category}' ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...")
                try:
                    self.driver.execute_script("window.scrollTo(0, 880);")
                    time.sleep(1)

                    cat_button_selector = f'a.issue-category[data-category="{category}"]'
                    cat_button = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, cat_button_selector)))
                    self.driver.execute_script("arguments[0].click();", cat_button)
                    time.sleep(4)

                    count = 0
                    for i in range(1, self.issues_per_category + 1):
                        try:
                            issue_selector = f'div.swiper-slide:nth-child({i}) .issue-item-link'
                            issue_element = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, issue_selector)))
                            
                            self.driver.execute_script("arguments[0].click();", issue_element)
                            time.sleep(2)

                            title_elem = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'p.issuPopTitle')))
                            content_elem = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'p.pT20.issuPopContent')))
                            
                            title = title_elem.text.strip()
                            content = content_elem.text.strip()

                            all_issues.append({
                                "ì´ìŠˆë²ˆí˜¸": total_issue_id,
                                "ì¹´í…Œê³ ë¦¬": category,
                                "ì œëª©": title,
                                "ë‚´ìš©": content,
                                "ì¶”ì¶œì‹œê°„": datetime.now().isoformat(),
                                "ê³ ìœ ID": f"{category}_{total_issue_id}"
                            })
                            total_issue_id += 1
                            count += 1
                            
                            ActionChains(self.driver).send_keys(Keys.ESCAPE).perform()
                            time.sleep(1)

                            if count >= self.issues_per_category:
                                break
                        
                        except (TimeoutException, NoSuchElementException) as e:
                            print(f"    - ì´ìŠˆ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                            try:
                                ActionChains(self.driver).send_keys(Keys.ESCAPE).perform()
                            except:
                                pass
                            continue
                    print(f"  -> '{category}' ì¹´í…Œê³ ë¦¬ì—ì„œ {count}ê°œ ì´ìŠˆ ìˆ˜ì§‘ ì™„ë£Œ.")

                except (TimeoutException, NoSuchElementException) as e:
                    print(f"âŒ '{category}' ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
        
        finally:
            self._cleanup_driver()
            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ. ì´ {len(all_issues)}ê°œì˜ ì´ìŠˆë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        
        return all_issues

# --- Service Singleton ---
_crawler_instance = BigKindsCrawler()

def crawl_news() -> List[Dict]:
    """ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜"""
    return _crawler_instance.crawl_all_categories()

def get_health() -> dict:
    return {"name": "crawling_service", "status": "ok"}


# --- í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ ---
if __name__ == '__main__':
    print("... crawling_service.py ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ...")
    
    # headless=Falseë¡œ ì„¤ì •í•˜ì—¬ ë¸Œë¼ìš°ì € ì°½ì„ ì§ì ‘ ë³´ë©´ì„œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    test_crawler = BigKindsCrawler(headless=False) 
    
    try:
        crawled_data = test_crawler.crawl_all_categories()
        
        if crawled_data:
            print("\n--- ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ (ìƒìœ„ 3ê°œ) ---")
            for i, issue in enumerate(crawled_data[:3]):
                print(f"  {i+1}. [{issue['ì¹´í…Œê³ ë¦¬']}] {issue['ì œëª©'][:50]}...")
            print(f"\nâœ… ì´ {len(crawled_data)}ê°œì˜ ì´ìŠˆë¥¼ ì„±ê³µì ìœ¼ë¡œ í¬ë¡¤ë§í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ì´ë‚˜ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    except Exception as e:
        import traceback
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()