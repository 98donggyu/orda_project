# services/crawling_service.py (ìˆ˜ì •ëœ ë²„ì „ - ì›ë³¸ ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
"""
í¬ë¡¤ë§ ë° í•„í„°ë§ í†µí•© ì„œë¹„ìŠ¤
ì›ë³¸ BigKindsCrawlerë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³  í•„í„°ë§ë§Œ ì¶”ê°€
"""

import os
import json
import time
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

# ì›ë³¸ BigKindsCrawler ê·¸ëŒ€ë¡œ import (ê°™ì€ í´ë”ì—ì„œ)
from .crawling_bigkinds import BigKindsCrawler

class CrawlingService:
    """í¬ë¡¤ë§ ë° í•„í„°ë§ í†µí•© ì„œë¹„ìŠ¤ - ì›ë³¸ BigKindsCrawler ì‚¬ìš©"""
    
    def __init__(self, data_dir: str = "data2", headless: bool = True):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.headless = headless
        
        load_dotenv(override=True)
        
        # AI í•„í„°ë§ìš© LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        
        print("âœ… í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def crawl_and_filter_news(self, 
                             issues_per_category: int = 10,
                             target_filtered_count: int = 5) -> Dict:
        """ì›ë³¸ BigKindsCrawler ì‚¬ìš© + í•„í„°ë§"""
        
        print(f"ğŸ•·ï¸ BigKinds í¬ë¡¤ë§ ì‹œì‘: ì¹´í…Œê³ ë¦¬ë³„ {issues_per_category}ê°œì”©")
        
        # Step 1: ì›ë³¸ BigKindsCrawlerë¡œ í¬ë¡¤ë§
        crawler = BigKindsCrawler(
            data_dir=str(self.data_dir),
            headless=self.headless,
            issues_per_category=issues_per_category
        )
        
        # ì›ë³¸ ë©”ì„œë“œ ê·¸ëŒ€ë¡œ í˜¸ì¶œ
        crawling_result = crawler.crawl_all_categories()
        
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {crawling_result.get('total_issues', 0)}ê°œ ì´ìŠˆ")
        
        # Step 2: í•„í„°ë§
        all_issues = crawling_result.get("all_issues", [])
        if all_issues:
            filtering_result = self._filter_by_stock_relevance(all_issues, target_filtered_count)
        else:
            filtering_result = {
                "selected_issues": [],
                "filter_metadata": {
                    "filtering_method": "no_issues_to_filter",
                    "original_count": 0,
                    "selected_count": 0,
                    "filtered_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
            }
        
        return {
            **crawling_result,
            "filtered_issues": filtering_result["selected_issues"],
            "filter_metadata": filtering_result["filter_metadata"]
        }
    
    def _filter_by_stock_relevance(self, all_issues: List[Dict], target_count: int) -> Dict:
        """ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„± ê¸°ë°˜ í•„í„°ë§"""
        
        print(f"ğŸ¤– AI í•„í„°ë§ ì‹œì‘: {len(all_issues)}ê°œ â†’ {target_count}ê°œ ì„ ë³„")
        
        # ê° ì´ìŠˆë³„ë¡œ ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        scored_issues = []
        
        for i, issue in enumerate(all_issues, 1):
            print(f"ğŸ”„ ì´ìŠˆ {i}/{len(all_issues)} ë¶„ì„ ì¤‘: {issue.get('ì œëª©', 'N/A')[:30]}...")
            
            # AIë¡œ ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„± ë¶„ì„
            relevance_score = self._analyze_stock_market_relevance(issue)
            
            scored_issue = issue.copy()
            scored_issue.update({
                "ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜": relevance_score["ì¢…í•©ì ìˆ˜"],
                "ê´€ë ¨ì„±_ë¶„ì„": relevance_score
            })
            
            scored_issues.append(scored_issue)
        
        # ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ ì„ ë³„
        scored_issues.sort(key=lambda x: x["ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜"], reverse=True)
        selected_issues = scored_issues[:target_count]
        
        # ìˆœìœ„ ë¶€ì—¬
        for rank, issue in enumerate(selected_issues, 1):
            issue["rank"] = rank
        
        result = {
            "selected_issues": selected_issues,
            "filter_metadata": {
                "filtering_method": "gpt-4o-mini_stock_relevance",
                "original_count": len(all_issues),
                "selected_count": len(selected_issues),
                "average_score": sum(issue["ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜"] for issue in selected_issues) / len(selected_issues) if selected_issues else 0,
                "filtered_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
        }
        
        # í•„í„°ë§ ê²°ê³¼ ì €ì¥
        self._save_filtering_result(result)
        
        print(f"âœ… AI í•„í„°ë§ ì™„ë£Œ: ìƒìœ„ {len(selected_issues)}ê°œ ì„ ë³„")
        return result
    
    def _analyze_stock_market_relevance(self, issue: Dict) -> Dict:
        """AIë¥¼ ì‚¬ìš©í•œ ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„± ë¶„ì„"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë„ˆëŠ” ë‰´ìŠ¤ê°€ ì£¼ì‹ì‹œì¥ì— ë¯¸ì¹  ì˜í–¥ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë‰´ìŠ¤ì˜ ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„±ì„ 1-10ì ìœ¼ë¡œ í‰ê°€í•´ì¤˜:

1. ì§ì ‘ì  ê¸°ì—… ì˜í–¥ (ê¸°ì—…ì‹¤ì , ê²½ì˜ì§„ ë³€í™” ë“±)
2. ì‚°ì—… ì „ë°˜ ì˜í–¥ (ì •ì±…ë³€í™”, ê¸°ìˆ í˜ì‹  ë“±) 
3. ê±°ì‹œê²½ì œ ì˜í–¥ (ê¸ˆë¦¬, í™˜ìœ¨, ì •ì±… ë“±)
4. íˆ¬ìì‹¬ë¦¬ ì˜í–¥ (ì‹œì¥ íŠ¸ë Œë“œ, ì´ìŠˆ í™•ì‚°ì„± ë“±)

ê° ê¸°ì¤€ë³„ ì ìˆ˜ì™€ ì¢…í•©ì ìˆ˜ë¥¼ ì œì‹œí•´ì¤˜."""),
            ("human", """
[ë‰´ìŠ¤ ì œëª©]
{title}

[ë‰´ìŠ¤ ë‚´ìš©]  
{content}

ìœ„ ë‰´ìŠ¤ì˜ ì£¼ì‹ì‹œì¥ ê´€ë ¨ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ (JSON):
{{
  "ì§ì ‘ì _ê¸°ì—…ì˜í–¥": ì ìˆ˜,
  "ì‚°ì—…_ì „ë°˜ì˜í–¥": ì ìˆ˜, 
  "ê±°ì‹œê²½ì œ_ì˜í–¥": ì ìˆ˜,
  "íˆ¬ìì‹¬ë¦¬_ì˜í–¥": ì ìˆ˜,
  "ì¢…í•©ì ìˆ˜": ì ìˆ˜,
  "ë¶„ì„ê·¼ê±°": "ìƒì„¸ ë¶„ì„ ë‚´ìš©"
}}""")
        ])
        
        parser = JsonOutputParser()
        chain = prompt | self.llm | parser
        
        try:
            result = chain.invoke({
                "title": issue.get("ì œëª©", ""),
                "content": issue.get("ë‚´ìš©", "")  # ì›ë³¸ì—ì„œëŠ” "ë‚´ìš©" í•„ë“œ ì‚¬ìš©
            })
            
            return {
                "ì§ì ‘ì _ê¸°ì—…ì˜í–¥": result.get("ì§ì ‘ì _ê¸°ì—…ì˜í–¥", 5),
                "ì‚°ì—…_ì „ë°˜ì˜í–¥": result.get("ì‚°ì—…_ì „ë°˜ì˜í–¥", 5),
                "ê±°ì‹œê²½ì œ_ì˜í–¥": result.get("ê±°ì‹œê²½ì œ_ì˜í–¥", 5), 
                "íˆ¬ìì‹¬ë¦¬_ì˜í–¥": result.get("íˆ¬ìì‹¬ë¦¬_ì˜í–¥", 5),
                "ì¢…í•©ì ìˆ˜": result.get("ì¢…í•©ì ìˆ˜", 5),
                "ë¶„ì„ê·¼ê±°": result.get("ë¶„ì„ê·¼ê±°", "AI ë¶„ì„ ì™„ë£Œ")
            }
            
        except Exception as e:
            print(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "ì§ì ‘ì _ê¸°ì—…ì˜í–¥": 5,
                "ì‚°ì—…_ì „ë°˜ì˜í–¥": 5,
                "ê±°ì‹œê²½ì œ_ì˜í–¥": 5,
                "íˆ¬ìì‹¬ë¦¬_ì˜í–¥": 5,
                "ì¢…í•©ì ìˆ˜": 5,
                "ë¶„ì„ê·¼ê±°": f"AI ë¶„ì„ ì‹¤íŒ¨: {e}"
            }
    
    def _save_filtering_result(self, result: Dict):
        """í•„í„°ë§ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y.%m.%d_%H.%M.%S")
        filename = f"{timestamp}_StockFiltered_{len(result['selected_issues'])}issues.json"
        filepath = self.data_dir / filename
        
        save_data = {
            **result,
            "file_info": {
                "filename": filename,
                "created_at": datetime.now().isoformat(),
                "filter_version": "StockRelevanceFilter_v1.0"
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ í•„í„°ë§ ê²°ê³¼ ì €ì¥: {filepath}")