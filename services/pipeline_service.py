"""
íŒŒì´í”„ë¼ì¸ í†µí•© ì„œë¹„ìŠ¤ - í¬ë¡¤ë§, í•„í„°ë§, RAG ë¶„ì„ì„ ì—°ê²°
integrated_pipeline.pyì˜ IntegratedNewsPipeline ë¡œì§ ì´ê´€
"""

import json
from pathlib import Path
from typing import Dict, List
from datetime import datetime
from .crawling_service import CrawlingService
from .rag_service import RAGService

class PipelineService:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© ì„œë¹„ìŠ¤"""
    
    def __init__(self, data_dir: str = "data2", headless: bool = True):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.crawling_service = CrawlingService(str(self.data_dir), headless)
        self.rag_service = RAGService()
        
        print("âœ… íŒŒì´í”„ë¼ì¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def execute_full_pipeline(self, 
                            issues_per_category: int = 10,
                            target_filtered_count: int = 5) -> Dict:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: í¬ë¡¤ë§ â†’ í•„í„°ë§ â†’ RAG ë¶„ì„"""
        
        pipeline_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        started_at = datetime.now()
        
        print(f"ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ (ID: {pipeline_id})")
        print(f"ğŸ“‹ ì„¤ì •: ì¹´í…Œê³ ë¦¬ë³„ {issues_per_category}ê°œ, ìµœì¢… ì„ ë³„ {target_filtered_count}ê°œ")
        
        result = {
            "pipeline_id": pipeline_id,
            "started_at": started_at.strftime("%Y-%m-%d %H:%M:%S"),
            "completed_at": None,
            "execution_time": None,
            "final_status": "running",
            "steps_completed": [],
            "errors": []
        }
        
        try:
            # Step 1: í¬ë¡¤ë§ + í•„í„°ë§
            print(f"\n{'='*60}")
            print(f"ğŸ“¡ Step 1: í¬ë¡¤ë§ + ì£¼ì‹ì‹œì¥ í•„í„°ë§")
            print(f"{'='*60}")
            
            crawling_result = self.crawling_service.crawl_and_filter_news(
                issues_per_category, target_filtered_count
            )
            
            result["crawling_result"] = {
                "total_crawled": len(crawling_result.get("all_issues", [])),
                "filtered_count": len(crawling_result.get("filtered_issues", []))
            }
            result["steps_completed"].append("crawling_and_filtering")
            
            # Step 2: RAG ë¶„ì„
            print(f"\n{'='*60}")
            print(f"ğŸ” Step 2: RAG ë¶„ì„ (ì‚°ì—… + ê³¼ê±° ì´ìŠˆ)")
            print(f"{'='*60}")
            
            filtered_issues = crawling_result.get("filtered_issues", [])
            if not filtered_issues:
                raise ValueError("í•„í„°ë§ëœ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            enriched_issues = self.rag_service.analyze_issues_with_rag(filtered_issues)
            
            result["rag_result"] = {
                "analyzed_count": len(enriched_issues),
                "average_confidence": self._calculate_average_confidence(enriched_issues)
            }
            result["steps_completed"].append("rag_analysis")
            
            # Step 3: APIìš© ë°ì´í„° ì¤€ë¹„
            print(f"\n{'='*60}")
            print(f"ğŸŒ Step 3: API ì‘ë‹µ ë°ì´í„° ì¤€ë¹„")
            print(f"{'='*60}")
            
            api_data = self._prepare_api_data(crawling_result, enriched_issues)
            result["api_ready_data"] = api_data
            result["steps_completed"].append("api_preparation")
            
            # íŒŒì´í”„ë¼ì¸ ì™„ë£Œ
            completed_at = datetime.now()
            execution_time = completed_at - started_at
            
            result.update({
                "completed_at": completed_at.strftime("%Y-%m-%d %H:%M:%S"),
                "execution_time": str(execution_time),
                "final_status": "success"
            })
            
            # ê²°ê³¼ ì €ì¥
            saved_file = self._save_pipeline_result(result)
            result["saved_file"] = saved_file
            
            print(f"\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
            print(f"â° ì‹¤í–‰ ì‹œê°„: {execution_time}")
            print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {len(enriched_issues)}ê°œ ì´ìŠˆ ë¶„ì„ ì™„ë£Œ")
            
            return result
            
        except Exception as e:
            error_msg = f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}"
            print(f"âŒ {error_msg}")
            
            result.update({
                "completed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "final_status": "failed",
                "errors": [error_msg]
            })
            
            raise Exception(error_msg)
    
    def _prepare_api_data(self, crawling_result: Dict, enriched_issues: List[Dict]) -> Dict:
        """API ì‘ë‹µìš© ë°ì´í„° êµ¬ì„±"""
        
        api_data = {
            "success": True,
            "data": {
                "total_crawled": len(crawling_result.get("all_issues", [])),
                "selected_count": len(enriched_issues),
                "selection_criteria": "ì£¼ì‹ì‹œì¥ ì˜í–¥ë„ + RAG ë¶„ì„",
                "selected_issues": []
            },
            "metadata": {
                "crawled_at": crawling_result.get("crawling_metadata", {}).get("timestamp", ""),
                "categories_processed": crawling_result.get("crawling_metadata", {}).get("categories_processed", []),
                "ai_filter_applied": True,
                "rag_analysis_applied": True,
                "filter_model": "gpt-4o-mini",
                "rag_model": "gpt-4o-mini",
                "rag_confidence": self._calculate_average_confidence(enriched_issues)
            }
        }
        
        # ì´ìŠˆ ë°ì´í„° ë³€í™˜
        for issue in enriched_issues:
            api_issue = {
                "ì´ìŠˆë²ˆí˜¸": issue.get("ì´ìŠˆë²ˆí˜¸", 0),
                "ì œëª©": issue.get("ì œëª©", ""),
                "ë‚´ìš©": issue.get("ì›ë³¸ë‚´ìš©", issue.get("ë‚´ìš©", "")),
                "ì¹´í…Œê³ ë¦¬": issue.get("ì¹´í…Œê³ ë¦¬", ""),
                "ì¶”ì¶œì‹œê°„": issue.get("ì¶”ì¶œì‹œê°„", ""),
                "ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜": issue.get("ì£¼ì‹ì‹œì¥_ê´€ë ¨ì„±_ì ìˆ˜", 0),
                "ìˆœìœ„": issue.get("rank", 0),
                
                # RAG ë¶„ì„ ê²°ê³¼
                "ê´€ë ¨ì‚°ì—…": issue.get("ê´€ë ¨ì‚°ì—…", []),
                "ê´€ë ¨ê³¼ê±°ì´ìŠˆ": issue.get("ê´€ë ¨ê³¼ê±°ì´ìŠˆ", []),
                "RAGë¶„ì„ì‹ ë¢°ë„": issue.get("RAGë¶„ì„ì‹ ë¢°ë„", 0.0),
            }
            api_data["data"]["selected_issues"].append(api_issue)
        
        # ìˆœìœ„ë³„ ì •ë ¬
        api_data["data"]["selected_issues"].sort(key=lambda x: x.get("ìˆœìœ„", 999))
        
        return api_data
    
    def _calculate_average_confidence(self, enriched_issues: List[Dict]) -> float:
        """í‰ê·  RAG ì‹ ë¢°ë„ ê³„ì‚°"""
        if not enriched_issues:
            return 0.0
        
        confidences = [issue.get("RAGë¶„ì„ì‹ ë¢°ë„", 0.0) for issue in enriched_issues]
        return round(sum(confidences) / len(confidences), 2)
    
    def _save_pipeline_result(self, result: Dict) -> str:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y.%m.%d_%H.%M.%S")
            filename = f"{timestamp}_Pipeline_Results.json"
            filepath = self.data_dir / filename
            
            save_data = {
                **result,
                "file_info": {
                    "filename": filename,
                    "created_at": datetime.now().isoformat(),
                    "pipeline_version": "PipelineService_v1.0"
                }
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì €ì¥: {filepath}")
            return str(filepath)
            
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def get_latest_analyzed_issues(self) -> List[Dict]:
        """ìµœì‹  ë¶„ì„ëœ ì´ìŠˆë“¤ ì¡°íšŒ (APIìš©)"""
        try:
            # 1. MySQLì—ì„œ ë¨¼ì € ì¡°íšŒ ì‹œë„
            from .database_service import DatabaseService
            db_service = DatabaseService()
            
            if db_service.is_initialized():
                mysql_data = db_service.get_latest_news_issues()
                if mysql_data:
                    print(f"ğŸ“Š MySQLì—ì„œ {len(mysql_data)}ê°œ ì´ìŠˆ ì¡°íšŒ")
                    return mysql_data
            
            # 2. MySQLì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìµœì‹  íŒŒì¼ì—ì„œ ì¡°íšŒ
            pipeline_files = list(self.data_dir.glob("*_Pipeline_Results.json"))
            if pipeline_files:
                latest_file = max(pipeline_files, key=lambda f: f.stat().st_mtime)
                
                with open(latest_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                api_data = data.get("api_ready_data", {})
                issues = api_data.get("data", {}).get("selected_issues", [])
                
                print(f"ğŸ“‚ íŒŒì¼ì—ì„œ {len(issues)}ê°œ ì´ìŠˆ ì¡°íšŒ: {latest_file.name}")
                return issues
            
            print("âš ï¸ ë¶„ì„ëœ ì´ìŠˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
            
        except Exception as e:
            print(f"âŒ ìµœì‹  ì´ìŠˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []