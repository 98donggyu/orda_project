# services/rag_service.py (ì™„ì „ ìˆ˜ì •ëœ ë²„ì „)
"""
RAG ë¶„ì„ ì„œë¹„ìŠ¤ - ë²¡í„° ê²€ìƒ‰ + AI Agent í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„
integrated_pipeline.pyì˜ RealRAGAnalysisExecutor ë¡œì§ ì´ê´€
"""

import os
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

class RAGService:
    """RAG ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        load_dotenv(override=True)
        
        # í™˜ê²½ ì„¤ì •
        self.EMBEDDING_MODEL = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
        self.INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "ordaproject")
        
        # LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.embedding = OpenAIEmbeddings(model=self.EMBEDDING_MODEL)
        
        # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self.industry_store = PineconeVectorStore(
            index_name=self.INDEX_NAME,
            embedding=self.embedding,
            namespace="industry"
        )
        
        self.past_issue_store = PineconeVectorStore(
            index_name=self.INDEX_NAME,
            embedding=self.embedding,
            namespace="past_issue"
        )
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”©
        self._load_databases()
        
        print("âœ… RAG ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_databases(self):
        """ì‚°ì—… DB ë° ê³¼ê±° ì´ìŠˆ DB ë¡œë”©"""
        try:
            # ì‚°ì—… DB ë¡œë”©
            self.industry_df = pd.read_csv("data/ì‚°ì—…DB.v.0.3.csv")
            self.industry_dict = dict(zip(self.industry_df["KRX ì—…ì¢…ëª…"], self.industry_df["ìƒì„¸ë‚´ìš©"]))
            self.valid_krx_names = list(self.industry_df["KRX ì—…ì¢…ëª…"].unique())
            print(f"âœ… ì‚°ì—… DB ë¡œë“œ: {len(self.valid_krx_names)}ê°œ ì—…ì¢…")
            
            # ê³¼ê±° ì´ìŠˆ DB ë¡œë”©
            self.past_df = pd.read_csv("data/Past_news.csv")
            self.issue_dict = dict(zip(
                self.past_df["Issue_name"], 
                self.past_df["Contents"] + "\n\nìƒì„¸: " + self.past_df["Contentes(Spec)"]
            ))
            self.valid_issue_names = list(self.past_df["Issue_name"].unique())
            print(f"âœ… ê³¼ê±° ì´ìŠˆ DB ë¡œë“œ: {len(self.valid_issue_names)}ê°œ ì´ìŠˆ")
            
        except Exception as e:
            print(f"âš ï¸ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.industry_dict = {}
            self.valid_krx_names = []
            self.issue_dict = {}
            self.valid_issue_names = []
    
    def analyze_issues_with_rag(self, filtered_issues: List[Dict]) -> List[Dict]:
        """í•„í„°ë§ëœ ì´ìŠˆë“¤ì— ëŒ€í•´ RAG ë¶„ì„ ìˆ˜í–‰"""
        
        print(f"ğŸ” RAG ë¶„ì„ ì‹œì‘: {len(filtered_issues)}ê°œ ì´ìŠˆ")
        
        enriched_issues = []
        
        for i, issue in enumerate(filtered_issues, 1):
            print(f"ğŸ”„ ì´ìŠˆ {i}/{len(filtered_issues)} RAG ë¶„ì„ ì¤‘: {issue.get('ì œëª©', 'N/A')[:50]}...")
            
            # ê´€ë ¨ ì‚°ì—… ë¶„ì„
            related_industries = self._analyze_industry_for_issue(issue)
            
            # ê´€ë ¨ ê³¼ê±° ì´ìŠˆ ë¶„ì„
            related_past_issues = self._analyze_past_issues_for_issue(issue)
            
            # RAG ì‹ ë¢°ë„ ê³„ì‚°
            rag_confidence = self._calculate_rag_confidence(related_industries, related_past_issues)
            
            # ê¸°ë³¸ ì´ìŠˆì— RAG ê²°ê³¼ ì¶”ê°€
            enriched_issue = issue.copy()
            enriched_issue.update({
                "ê´€ë ¨ì‚°ì—…": related_industries,
                "ê´€ë ¨ê³¼ê±°ì´ìŠˆ": related_past_issues,
                "RAGë¶„ì„ì‹ ë¢°ë„": rag_confidence
            })
            
            enriched_issues.append(enriched_issue)
            
            print(f"   âœ… ì´ìŠˆ {i} RAG ì™„ë£Œ: ì‚°ì—… {len(related_industries)}ê°œ, ê³¼ê±°ì´ìŠˆ {len(related_past_issues)}ê°œ, ì‹ ë¢°ë„ {rag_confidence}")
        
        print(f"âœ… RAG ë¶„ì„ ì™„ë£Œ: í‰ê·  ì‹ ë¢°ë„ {self._calculate_average_confidence(enriched_issues)}")
        return enriched_issues
    
    def _analyze_industry_for_issue(self, issue: Dict) -> List[Dict]:
        """íŠ¹ì • ì´ìŠˆì— ëŒ€í•œ ê´€ë ¨ ì‚°ì—… ë¶„ì„"""
        try:
            query = f"{issue.get('ì œëª©', '')}\n{issue.get('ì›ë³¸ë‚´ìš©', issue.get('ë‚´ìš©', ''))}"
            
            # Step 1: ë²¡í„° ê²€ìƒ‰
            vector_candidates = self._vector_search_industries(query)
            
            # Step 2: AI Agent ë¶„ì„
            ai_candidates = self._ai_extract_candidate_industries(query)
            
            # Step 3: ê²°ê³¼ í†µí•©
            final_candidates = self._combine_industry_results(query, vector_candidates, ai_candidates)
            
            return final_candidates[:3]  # ìƒìœ„ 3ê°œ ë°˜í™˜
            
        except Exception as e:
            print(f"âŒ ì‚°ì—… ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _analyze_past_issues_for_issue(self, issue: Dict) -> List[Dict]:
        """íŠ¹ì • ì´ìŠˆì— ëŒ€í•œ ê´€ë ¨ ê³¼ê±° ì´ìŠˆ ë¶„ì„"""
        try:
            query = f"{issue.get('ì œëª©', '')}\n{issue.get('ì›ë³¸ë‚´ìš©', issue.get('ë‚´ìš©', ''))}"
            
            # Step 1: ë²¡í„° ê²€ìƒ‰
            vector_candidates = self._vector_search_past_issues(query)
            
            # Step 2: AI Agent ë¶„ì„  
            ai_candidates = self._ai_extract_candidate_past_issues(query)
            
            # Step 3: ê²°ê³¼ í†µí•©
            final_candidates = self._combine_past_issue_results(query, vector_candidates, ai_candidates)
            
            return final_candidates[:3]  # ìƒìœ„ 3ê°œ ë°˜í™˜
            
        except Exception as e:
            print(f"âŒ ê³¼ê±° ì´ìŠˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _vector_search_industries(self, query: str) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì‚°ì—… í›„ë³´ ì¶”ì¶œ"""
        try:
            results = self.industry_store.similarity_search_with_score(query, k=10)
            
            candidates = []
            for doc, score in results:
                content = doc.page_content.replace('\ufeff', '').replace('ï»¿', '')
                
                if "KRX ì—…ì¢…ëª…:" in content:
                    lines = content.split("\n")
                    for line in lines:
                        if "KRX ì—…ì¢…ëª…:" in line:
                            industry_name = line.replace("KRX ì—…ì¢…ëª…:", "").strip()
                            if industry_name in self.industry_dict:
                                if not any(c["name"] == industry_name for c in candidates):
                                    similarity_percentage = round((1 - score) * 100, 1)
                                    
                                    content_parts = content.split("ìƒì„¸ë‚´ìš©:")
                                    industry_detail = content_parts[1].strip() if len(content_parts) > 1 else self.industry_dict[industry_name]
                                    
                                    candidates.append({
                                        "name": industry_name,
                                        "similarity": similarity_percentage,
                                        "description": industry_detail
                                    })
                            break
            
            return candidates
            
        except Exception as e:
            print(f"âŒ ì‚°ì—… ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _vector_search_past_issues(self, query: str) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ê³¼ê±° ì´ìŠˆ í›„ë³´ ì¶”ì¶œ"""
        try:
            results = self.past_issue_store.similarity_search_with_score(query, k=10)
            
            candidates = []
            for doc, score in results:
                content = doc.page_content.replace('\ufeff', '').replace('ï»¿', '')
                
                if "Issue_name:" in content:
                    lines = content.split("\n")
                    for line in lines:
                        if "Issue_name:" in line:
                            issue_name = line.replace("Issue_name:", "").strip()
                            if issue_name in self.issue_dict:
                                if not any(c["name"] == issue_name for c in candidates):
                                    similarity_percentage = round((1 - score) * 100, 1)
                                    
                                    content_parts = content.split("Contents:")
                                    issue_detail = content_parts[1].strip() if len(content_parts) > 1 else self.issue_dict[issue_name]
                                    
                                    # ê¸°ê°„ ì •ë³´ ì¶”ì¶œ
                                    period = "N/A"
                                    for line in lines:
                                        if "Start_date:" in line and "Fin_date:" in line:
                                            start = line.split("Start_date:")[1].split("Fin_date:")[0].strip()
                                            end = line.split("Fin_date:")[1].strip()
                                            period = f"{start} ~ {end}"
                                            break
                                    
                                    candidates.append({
                                        "name": issue_name,
                                        "similarity": similarity_percentage,
                                        "description": issue_detail,
                                        "period": period
                                    })
                            break
            
            return candidates
            
        except Exception as e:
            print(f"âŒ ê³¼ê±° ì´ìŠˆ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _ai_extract_candidate_industries(self, news_content: str) -> List[Dict]:
        """AI Agentê°€ ê´€ë ¨ ì‚°ì—… í›„ë³´ ì¶”ì¶œ"""
        if not self.valid_krx_names:
            return []
            
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë„ˆëŠ” ë‰´ìŠ¤ì™€ ì‚°ì—…ì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•˜ëŠ” ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì•¼.
ì£¼ì–´ì§„ ë‰´ìŠ¤ ë‚´ìš©ì„ ë¶„ì„í•˜ê³ , ì œê³µëœ KRX ì—…ì¢… ë¦¬ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì‚°ì—…ë“¤ì„ ì„ ë³„í•´ì•¼ í•´.

ê´€ë ¨ì„± íŒë‹¨ ê¸°ì¤€:
1. ì§ì ‘ì  ì˜í–¥: ë‰´ìŠ¤ê°€ í•´ë‹¹ ì‚°ì—…ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?
2. ê³µê¸‰ë§ ê´€ê³„: ë‰´ìŠ¤ ê´€ë ¨ ê¸°ì—…/ì‚°ì—…ê³¼ ê³µê¸‰ë§ ê´€ê³„ê°€ ìˆëŠ”ê°€?
3. ì‹œì¥ ë™í–¥: ë‰´ìŠ¤ê°€ í•´ë‹¹ ì‚°ì—…ì˜ ì‹œì¥ ë™í–¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?
4. ì •ì±…/ê·œì œ: ë‰´ìŠ¤ê°€ í•´ë‹¹ ì‚°ì—… ê´€ë ¨ ì •ì±…ì´ë‚˜ ê·œì œì™€ ì—°ê´€ë˜ëŠ”ê°€?"""),
            ("human", """
[ë‰´ìŠ¤ ë‚´ìš©]
{news}

[KRX ì—…ì¢… ë¦¬ìŠ¤íŠ¸]  
{industries}

ìœ„ ë‰´ìŠ¤ì™€ ê´€ë ¨ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì‚°ì—…ì„ 10ê°œ ì„ ë³„í•´ì£¼ì„¸ìš”.
ê° ì‚°ì—…ì— ëŒ€í•´ ê´€ë ¨ì„± ì ìˆ˜(1-10ì )ì™€ ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ (JSON):
{{
  "candidates": [
    {{"industry": "ì‚°ì—…ëª…", "score": ì ìˆ˜, "reason": "ê´€ë ¨ì„± ì´ìœ "}},
    ...
  ]
}}""")
        ])
        
        parser = JsonOutputParser()
        chain = prompt | self.llm | parser
        
        try:
            result = chain.invoke({
                "news": news_content,
                "industries": ", ".join(self.valid_krx_names[:50])  # ë„ˆë¬´ ë§ìœ¼ë©´ ì œí•œ
            })
            return result.get("candidates", [])
        except Exception as e:
            print(f"âŒ AI ì‚°ì—… í›„ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")  # ğŸ”§ ìˆ˜ì •: ì—ëŸ¬ ë©”ì‹œì§€ ì˜¬ë°”ë¥´ê²Œ ë³€ê²½
            return []
    
    def _ai_extract_candidate_past_issues(self, news_content: str) -> List[Dict]:
        """AI Agentê°€ ê´€ë ¨ ê³¼ê±° ì´ìŠˆ í›„ë³´ ì¶”ì¶œ"""  # ğŸ”§ ì¶”ê°€: ë¹ ì ¸ìˆë˜ í•¨ìˆ˜ ì¶”ê°€
        if not self.valid_issue_names:
            return []
            
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë„ˆëŠ” í˜„ì¬ ë‰´ìŠ¤ì™€ ê³¼ê±° ì´ìŠˆì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•˜ëŠ” ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì•¼.
ì£¼ì–´ì§„ í˜„ì¬ ë‰´ìŠ¤ ë‚´ìš©ì„ ë¶„ì„í•˜ê³ , ì œê³µëœ ê³¼ê±° ì´ìŠˆ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì´ìŠˆë“¤ì„ ì„ ë³„í•´ì•¼ í•´.

ê´€ë ¨ì„± íŒë‹¨ ê¸°ì¤€:
1. ìœ ì‚¬í•œ ì‹œì¥ ìƒí™©: ê³¼ê±° ì´ìŠˆì™€ í˜„ì¬ ìƒí™©ì´ ìœ ì‚¬í•œ ì‹œì¥ í™˜ê²½ì¸ê°€?
2. ë™ì¼í•œ ì‚°ì—…/ê¸°ì—… ì˜í–¥: ê°™ì€ ì‚°ì—…ì´ë‚˜ ìœ ì‚¬í•œ ê¸°ì—…ë“¤ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?
3. ì •ì±…/ê²½ì œì  ìœ ì‚¬ì„±: ì •ì±… ë³€í™”ë‚˜ ê²½ì œì  ìš”ì¸ì´ ìœ ì‚¬í•œê°€?
4. íˆ¬ìì ì‹¬ë¦¬: íˆ¬ììë“¤ì˜ ë°˜ì‘ì´ë‚˜ ì‹œì¥ ì‹¬ë¦¬ê°€ ë¹„ìŠ·í•œê°€?"""),
            ("human", """
[í˜„ì¬ ë‰´ìŠ¤ ë‚´ìš©]
{news}

[ê³¼ê±° ì´ìŠˆ ë¦¬ìŠ¤íŠ¸]
{issues}

ìœ„ í˜„ì¬ ë‰´ìŠ¤ì™€ ê´€ë ¨ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê³¼ê±° ì´ìŠˆë¥¼ 10ê°œ ì„ ë³„í•´ì£¼ì„¸ìš”.
ê° ê³¼ê±° ì´ìŠˆì— ëŒ€í•´ ê´€ë ¨ì„± ì ìˆ˜(1-10ì )ì™€ ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ (JSON):
{{
  "candidates": [
    {{"issue": "ì´ìŠˆëª…", "score": ì ìˆ˜, "reason": "ê´€ë ¨ì„± ì´ìœ "}},
    ...
  ]
}}""")
        ])
        
        parser = JsonOutputParser()
        chain = prompt | self.llm | parser
        
        try:
            result = chain.invoke({
                "news": news_content,
                "issues": ", ".join(self.valid_issue_names[:50])  # ë„ˆë¬´ ë§ìœ¼ë©´ ì œí•œ
            })
            return result.get("candidates", [])
        except Exception as e:
            print(f"âŒ AI ê³¼ê±° ì´ìŠˆ í›„ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _combine_industry_results(self, query: str, vector_candidates: List[Dict], ai_candidates: List[Dict]) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ì™€ AI í›„ë³´ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ê´€ë ¨ ì‚°ì—… ë„ì¶œ"""
        all_candidates = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for candidate in vector_candidates:
            name = candidate["name"]
            all_candidates[name] = {
                "name": name,
                "vector_similarity": candidate["similarity"],
                "ai_score": 0,
                "ai_reason": "",
                "description": candidate["description"]
            }
        
        # AI í›„ë³´ ì¶”ê°€/ì—…ë°ì´íŠ¸
        for candidate in ai_candidates:
            name = candidate["industry"]
            if name in all_candidates:
                all_candidates[name]["ai_score"] = candidate["score"]
                all_candidates[name]["ai_reason"] = candidate["reason"]
            elif name in self.industry_dict:  # ìœ íš¨í•œ ì‚°ì—…ëª…ì¸ ê²½ìš°ë§Œ
                all_candidates[name] = {
                    "name": name,
                    "vector_similarity": 0,
                    "ai_score": candidate["score"],
                    "ai_reason": candidate["reason"],
                    "description": self.industry_dict[name]
                }
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ë²¡í„° ìœ ì‚¬ë„ + AI ì ìˆ˜)
        for candidate in all_candidates.values():
            # ë²¡í„° ìœ ì‚¬ë„ë¥¼ 10ì  ë§Œì ìœ¼ë¡œ ì •ê·œí™”
            normalized_vector = candidate["vector_similarity"] / 10
            ai_score = candidate["ai_score"]
            
            # ê°€ì¤‘í‰ê·  (AI ì ìˆ˜ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            candidate["final_score"] = round((normalized_vector * 0.3 + ai_score * 0.7), 1)
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        sorted_candidates = sorted(all_candidates.values(), 
                                  key=lambda x: x["final_score"], 
                                  reverse=True)
        
        return sorted_candidates
    
    def _combine_past_issue_results(self, query: str, vector_candidates: List[Dict], ai_candidates: List[Dict]) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ì™€ AI í›„ë³´ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ê´€ë ¨ ê³¼ê±° ì´ìŠˆ ë„ì¶œ"""
        all_candidates = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for candidate in vector_candidates:
            name = candidate["name"]
            all_candidates[name] = {
                "name": name,
                "vector_similarity": candidate["similarity"],
                "ai_score": 0,
                "ai_reason": "",
                "description": candidate["description"],
                "period": candidate.get("period", "N/A")
            }
        
        # AI í›„ë³´ ì¶”ê°€/ì—…ë°ì´íŠ¸
        for candidate in ai_candidates:
            name = candidate["issue"]
            if name in all_candidates:
                all_candidates[name]["ai_score"] = candidate["score"]
                all_candidates[name]["ai_reason"] = candidate["reason"]
            elif name in self.issue_dict:  # ìœ íš¨í•œ ì´ìŠˆëª…ì¸ ê²½ìš°ë§Œ
                all_candidates[name] = {
                    "name": name,
                    "vector_similarity": 0,
                    "ai_score": candidate["score"],
                    "ai_reason": candidate["reason"],
                    "description": self.issue_dict[name],
                    "period": "N/A"
                }
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        for candidate in all_candidates.values():
            normalized_vector = candidate["vector_similarity"] / 10
            ai_score = candidate["ai_score"]
            candidate["final_score"] = round((normalized_vector * 0.3 + ai_score * 0.7), 1)
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        sorted_candidates = sorted(all_candidates.values(), 
                                    key=lambda x: x["final_score"], 
                                    reverse=True)
        
        return sorted_candidates
    
    def _calculate_rag_confidence(self, industries: List[Dict], past_issues: List[Dict]) -> float:
        """RAG ë¶„ì„ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not industries or not past_issues:
            return 0.0
        
        # ì‹¤ì œ final_score ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
        industry_avg = sum(ind.get("final_score", 0) for ind in industries) / len(industries)
        past_avg = sum(issue.get("final_score", 0) for issue in past_issues) / len(past_issues)
        
        return round((industry_avg + past_avg) / 2, 1)
    
    def _calculate_average_confidence(self, enriched_issues: List[Dict]) -> float:
        """ì „ì²´ ì´ìŠˆë“¤ì˜ í‰ê·  RAG ì‹ ë¢°ë„ ê³„ì‚°"""
        if not enriched_issues:
            return 0.0
        
        confidences = [issue.get("RAGë¶„ì„ì‹ ë¢°ë„", 0.0) for issue in enriched_issues]
        return round(sum(confidences) / len(confidences), 2)